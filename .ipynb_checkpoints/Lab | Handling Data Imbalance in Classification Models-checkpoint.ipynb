{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97c12de",
   "metadata": {},
   "source": [
    "\n",
    "# Lab | Handling Data Imbalance in Classification Models\n",
    "\n",
    "For this lab and in the next lessons we will use the dataset 'Healthcare For All' building a model to predict who will donate (TargetB) and how much they will give (TargetD) (will be used for lab on Friday). You will be using `files_for_lab/learningSet.csv` file which you have already downloaded from class.\n",
    "\n",
    "### Scenario\n",
    "\n",
    "You are revisiting the Healthcare for All Case Study. You are provided with this historical data about Donors and how much they donated. Your task is to build a machine learning model that will help the company identify people who are more likely to donate and then try to predict the donation amount.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "In this lab, we will first take a look at the degree of imbalance in the data and correct it using the techniques we learned in the class.\n",
    "\n",
    "Here is the list of steps to be followed (building a simple model without balancing the data):\n",
    "\n",
    "- Import the required libraries and modules that you would need.\n",
    "- Read that data into Python and call the dataframe `donors`.\n",
    "- Check the datatypes of all the columns in the data. \n",
    "- Check for null values in the dataframe. Replace the null values using the methods learned in class.\n",
    "- Split the data into numerical and catagorical.  Decide if any columns need their dtype changed.\n",
    "- Concatenate numerical and categorical back together again for your X dataframe.  Designate the Target as y.\n",
    "  \n",
    "  - Split the data into a training set and a test set.\n",
    "  - Split further into train_num and train_cat.  Also test_num and test_cat.\n",
    "  - Scale the features either by using normalizer or a standard scaler. (train_num, test_num)\n",
    "  - Encode the categorical features using One-Hot Encoding or Ordinal Encoding.  (train_cat, test_cat)\n",
    "      - **fit** only on train data transform both train and test\n",
    "      - again re-concatenate train_num and train_cat as X_train as well as test_num and test_cat as X_test\n",
    "  - Fit a logistic regression model on the training data.\n",
    "  - Check the accuracy on the test data.\n",
    "\n",
    "**Note**: So far we have not balanced the data.\n",
    "\n",
    "Managing imbalance in the dataset\n",
    "\n",
    "- Check for the imbalance.\n",
    "- Use the resampling strategies used in class for upsampling and downsampling to create a balance between the two classes.\n",
    "- Each time fit the model and see how the accuracy of the model has changed.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4686811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ec01ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "375e0f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    90569\n",
      "1     4843\n",
      "Name: TARGET_B, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "numerical = pd.read_csv('/Users/leozinho.air/Desktop/Ironhack/class_30/numerical.csv')\n",
    "categorical = pd.read_csv('/Users/leozinho.air/Desktop/Ironhack/class_30/categorical.csv')\n",
    "targets = pd.read_csv('/Users/leozinho.air/Desktop/Ironhack/class_30/target.csv')\n",
    "donors = pd.concat([numerical, categorical, targets], axis = 1)\n",
    "print(donors['TARGET_B'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "74090c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCODE             int64\n",
       "AGE             float64\n",
       "INCOME            int64\n",
       "WEALTH1           int64\n",
       "HIT               int64\n",
       "MALEMILI          int64\n",
       "MALEVET           int64\n",
       "VIETVETS          int64\n",
       "WWIIVETS          int64\n",
       "LOCALGOV          int64\n",
       "STATEGOV          int64\n",
       "FEDGOV            int64\n",
       "                 ...   \n",
       "DOB_YR            int64\n",
       "DOB_MM            int64\n",
       "MINRDATE_YR       int64\n",
       "MINRDATE_MM       int64\n",
       "MAXRDATE_YR       int64\n",
       "MAXRDATE_MM       int64\n",
       "LASTDATE_YR       int64\n",
       "LASTDATE_MM       int64\n",
       "FIRSTDATE_YR      int64\n",
       "FIRSTDATE_MM    float64\n",
       "TARGET_B          int64\n",
       "TARGET_D        float64\n",
       "Length: 339, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dtypes\n",
    "donors.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f4b8f358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of null values: \n",
      "2\n",
      "\n",
      "\n",
      "Dataframe with % of null values per column sorted descending:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns_name</th>\n",
       "      <th>nulls_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>FIRSTDATE_MM</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCODE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>OEDC4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>EC5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>EC4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>EC3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>EC2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>EC1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>OEDC7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>HVP6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>HVP5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>HVP4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>HVP3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>HVP2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>HVP1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>ETHC6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>ETHC5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>TARGET_D</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>339 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     columns_name  nulls_percentage\n",
       "336  FIRSTDATE_MM          0.000021\n",
       "0           TCODE          0.000000\n",
       "223         OEDC4          0.000000\n",
       "231           EC5          0.000000\n",
       "230           EC4          0.000000\n",
       "229           EC3          0.000000\n",
       "228           EC2          0.000000\n",
       "227           EC1          0.000000\n",
       "226         OEDC7          0.000000\n",
       "..            ...               ...\n",
       "115          HVP6          0.000000\n",
       "114          HVP5          0.000000\n",
       "113          HVP4          0.000000\n",
       "112          HVP3          0.000000\n",
       "111          HVP2          0.000000\n",
       "110          HVP1          0.000000\n",
       "109         ETHC6          0.000000\n",
       "108         ETHC5          0.000000\n",
       "338      TARGET_D          0.000000\n",
       "\n",
       "[339 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for null\n",
    "print('Amount of null values: ')\n",
    "print(donors.isnull().sum().sum())\n",
    "print('\\n')\n",
    "\n",
    "# Do the percentage of nulls\n",
    "nulls_percent_df = donors.isna().sum()/len(donors)\n",
    "\n",
    "# Put it into a dataframe\n",
    "nulls_percent_df = pd.DataFrame(donors.isna().sum()/len(donors))\n",
    "\n",
    "# Take it out of the index\n",
    "nulls_percent_df = pd.DataFrame(donors.isna().sum()/len(donors)).reset_index()\n",
    "\n",
    "# Finally lets change columns names\n",
    "nulls_percent_df.columns = ['columns_name', 'nulls_percentage']\n",
    "\n",
    "# Order it\n",
    "sorted_nulls_percent_df = nulls_percent_df.sort_values(by='nulls_percentage', ascending=False)\n",
    "\n",
    "# Display the dataset\n",
    "print('Dataframe with % of null values per column sorted descending:')\n",
    "display(sorted_nulls_percent_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0467897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "donors.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "747aa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X y split \n",
    "y = donors['TARGET_B']\n",
    "X = donors.drop(['TARGET_B', 'TARGET_D'], axis = 1)\n",
    "\n",
    "# Split the data into numerical and categorical\n",
    "numericalX = X.select_dtypes(include = [np.number])\n",
    "categoricalX = X.select_dtypes(exclude = [np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "918cba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(drop='first').fit(categoricalX)\n",
    "encoded_categorical = encoder.transform(categoricalX).toarray()\n",
    "encoded_categorical = pd.DataFrame(encoded_categorical)\n",
    "encoded_categorical.columns = [str(col) if isinstance(col, int) else col for col in encoded_categorical.columns] # colums as strings\n",
    "\n",
    "# Normalizing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "numericalX_norm = min_max_scaler.fit_transform(numericalX)\n",
    "numericalX_norm = pd.DataFrame(numericalX_norm, columns=numericalX.columns)  # Ensure to use the original column names\n",
    "\n",
    "# Concat again the data\n",
    "X = pd.concat([numericalX_norm, encoded_categorical], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "555ab4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    72496\n",
      "1     3832\n",
      "Name: TARGET_B, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train = pd.concat([X_train, y_train],axis=1)\n",
    "print(train['TARGET_B'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "911bdd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9469657268630123\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on imbalanced data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter = 100)\n",
    "\n",
    "# Fit the model\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "# Predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\") # 0.946\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "318a7d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is :  0.06921086675291074\n",
      "Recall is :  0.5291790306627102\n",
      "F1 is :  0.12241162338405218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.60      0.74     18071\n",
      "           1       0.07      0.53      0.12      1011\n",
      "\n",
      "    accuracy                           0.60     19082\n",
      "   macro avg       0.51      0.57      0.43     19082\n",
      "weighted avg       0.91      0.60      0.71     19082\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The results of your logistic regression model on imbalanced data show a high accuracy of 0.946 (or 94.6%), \\nwhich initially might seem excellent. However, the precision, recall, and F1 scores for the minority class (class 1) are all 0.0,\\nindicating the model did not correctly predict any instances of the minority class.\\nThis is a common issue when working with imbalanced datasets, where the model becomes biased towards the majority class (class 0 in this case), \\nleading to poor performance on the minority class despite high overall accuracy.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score #precision metrics\n",
    "from sklearn.metrics import recall_score #recall metrics\n",
    "from sklearn.metrics import f1_score #f1 metrics\n",
    "from sklearn.metrics import classification_report #classification metrics\n",
    "\n",
    "#maximizing recall for all healthy issue topic\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Precision is : \", precision_score(y_test, pred))\n",
    "print(\"Recall is : \", recall_score(y_test, pred))\n",
    "print(\"F1 is : \", f1_score(y_test, pred))\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03de0b8",
   "metadata": {},
   "source": [
    "#### The results of the logistic regression model on imbalanced data show a high accuracy of 0.946 (or 94.6%), which initially might seem excellent. However, the precision, recall, and F1 scores for the minority class (class 1) are all 0.0, indicating the model did not correctly predict any instances of the minority class. This is a common issue when working with imbalanced datasets, where the model becomes biased towards the majority class (class 0 in this case), leading to poor performance on the minority class despite high overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62d6ac",
   "metadata": {},
   "source": [
    "## Managing imbalance in the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "788b3dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    72496\n",
       "1     3832\n",
       "Name: TARGET_B, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for the imbalance\n",
    "display(train['TARGET_B'].value_counts()) # 0 -> 72496 # 1 -> 3832\n",
    "\n",
    "# Separate majority and minority classes\n",
    "category_0 = train[train['TARGET_B'] == 0]\n",
    "category_1 = train[train['TARGET_B'] == 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd33f9f",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ad444b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3832\n",
       "1    3832\n",
       "Name: TARGET_B, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample the majority class\n",
    "from sklearn.utils import resample\n",
    "\n",
    "category_0_downsampled = resample(category_0, \n",
    "                                  replace=False,    # sample without replacement\n",
    "                                  n_samples=len(category_1),  # to match minority class\n",
    "                                  random_state=123) # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "train_downsampled = pd.concat([category_0_downsampled, category_1])\n",
    "train_downsampled = train_downsampled.reset_index(drop = True)\n",
    "\n",
    "train_downsampled[\"TARGET_B\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "42ca6768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6092118997912317\n"
     ]
    }
   ],
   "source": [
    "# X y split\n",
    "X_down = train_downsampled.drop(['TARGET_B'], axis = 1)\n",
    "y_down = train_downsampled['TARGET_B']\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter = 100)\n",
    "\n",
    "# Fit the model\n",
    "lr.fit(X_down,y_down)\n",
    "\n",
    "# Predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = lr.predict(X_down)\n",
    "accuracy = accuracy_score(y_down, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\") # 0.609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f31b6fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is :  0.6121082239485668\n",
      "Recall is :  0.596294363256785\n",
      "F1 is :  0.604097818902842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.62      0.61      3832\n",
      "           1       0.61      0.60      0.60      3832\n",
      "\n",
      "    accuracy                           0.61      7664\n",
      "   macro avg       0.61      0.61      0.61      7664\n",
      "weighted avg       0.61      0.61      0.61      7664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Precision is : \", precision_score(y_down, y_pred))\n",
    "print(\"Recall is : \", recall_score(y_down, y_pred))\n",
    "print(\"F1 is : \", f1_score(y_down, y_pred))\n",
    "\n",
    "print(classification_report(y_down, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe8352",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "82b8fd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    72496\n",
       "1    72496\n",
       "Name: TARGET_B, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_over = train.drop('TARGET_B', axis = 1)\n",
    "y_over = train['TARGET_B']\n",
    "\n",
    "X_sm, y_sm = smote.fit_resample(X_over, y_over)\n",
    "y_sm.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4b9d327e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6188548333701169\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter = 100)\n",
    "# Fit the model\n",
    "lr.fit(X_sm,y_sm)\n",
    "\n",
    "# Predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = lr.predict(X_sm)\n",
    "accuracy = accuracy_score(y_sm, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\") # 0.618\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c237c",
   "metadata": {},
   "source": [
    "#### In the downsampled scenario, the dataset size is reduced to make the classes balanced (each class having 3832 instances). The metrics here are relatively balanced across both classes, indicating that the model is performing fairly evenly in predicting both classes. This balance is crucial in scenarios where both false positives and false negatives carry significant consequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "70b2a637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is :  0.6168988861604418\n",
      "Recall is :  0.6272208121827412\n",
      "F1 is :  0.6220170308812969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.61      0.62     72496\n",
      "           1       0.62      0.63      0.62     72496\n",
      "\n",
      "    accuracy                           0.62    144992\n",
      "   macro avg       0.62      0.62      0.62    144992\n",
      "weighted avg       0.62      0.62      0.62    144992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Precision is : \", precision_score(y_sm, y_pred))\n",
    "print(\"Recall is : \", recall_score(y_sm, y_pred))\n",
    "print(\"F1 is : \", f1_score(y_sm, y_pred))\n",
    "\n",
    "print(classification_report(y_sm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1fc354",
   "metadata": {},
   "source": [
    "#### With the upsampled dataset, the minority class is increased to match the majority class, resulting in a much larger dataset (each class having 72,496 instances). The metrics show a slight improvement over the downsampled data, particularly in recall, which suggests the model is slightly better at identifying positive cases in the upsampled scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef746eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
